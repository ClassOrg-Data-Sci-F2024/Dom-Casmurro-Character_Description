---
title: "Data Processing - Dom Casmurro"
author: "LUIZA DIVINO"
date: "2024-10-27"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goal
Verify how Capitu and Escobar are described throughout the story, making a comparison between how they were portrayed by Bentinho before and after the apparent adultery

## Cleaning and reorganizing

- Load all the packages that will be needed for the analysis

- Import data: PDF (transform data to a df)

- Tokenize data: words

- Create columns: `text`, `word` and `page`

- Lowercase data

- Add stop wordlist without common words (https://gist.github.com/alopes/5358189) and create new dataset


### Load packages
```{r}
library(tidyverse)
library(tidytext)
library(pdftools)
library(tibble)
library (stringr)
library(dplyr)
library(ggplot2)
install.packages("ggwordcloud")
library(ggwordcloud)
# found about this package (could not use it)
library(lexiconPT)
```

### Import data and process data

The data is in PDF format, so first it is necessary to read the PDF file using `pdf_text`.
```{r}

dom_casmurro_pdf <- pdf_text("data/domCasmurro.pdf")

# use cat to have a preview of the first page
cat(dom_casmurro_pdf[1])
??cat

?udpipe_load_model
```

Change format so that it is possible to access the data. Transforming the pdf to tibble separates the whole content in each page as if they were one single value. For it to be possible to analyse the data, it is necessary to tokenize the corpus word by word, making it possible to access in which page each word is located. This will leave us with two variables: `page` and `word`.

Before going through this process, there were 128 observations (the book has 128 pages). The tokenization leaves us with 669267 observations, which represents the total number of words in the book, including elements like "Chapter", for example. 
```{r}
# data transformation to page
dom_casmurro_pages <- tibble(page = 1:length(dom_casmurro_pdf), text = dom_casmurro_pdf)

# data transformation to text for the pos tagger
dom_casmurro_text <- str_flatten(dom_casmurro_pages$text)

# data tokenization
dom_casmurro_words <- dom_casmurro_pages %>%
  unnest_tokens(word, text)


# naming the columns "page" and "word" for them to match the names of the columns on the stop wordlist
colnames(dom_casmurro_words) <- c("page", "word")
```

For future analysis, it is a good idea to lowercase the whole corpus. For doing so, `str_to_lower_` will be used.
```{r}
dom_casmurro_words <- dom_casmurro_words %>%
  mutate(word = str_to_lower(str_trim(word)))
```

Frequency list of words to have an overview of what have been used in the book.
```{r}
dom_casmurro_words %>% 
  count(word, sort = TRUE)
```

### Looking for context in which ""Capitu" and "Escobar" 

#### Capitu
As a starting point, I have focused on "Capitu", since her name is the first content word that shows on the list with the most frequent words (I had to look for it manually after using `count()`).


"Finding" Capitu
`str_view()` gives the whole page. This is not practical because it gives to much information, but it is a good mechanism to look for more context when needed.
```{r, include=FALSE}
str_view(dom_casmurro_pages$text, "Capitu")
```

Context after Capitu (up to 10 words)
I tested a lot of different lengths and I might go with 5.
```{r}
# str_extract_all gives access to all
# filter(is.na!) removes entries without capitu

dom_casmurro_pages %>%
  mutate(word_after_capitu = str_extract_all(text, "Capitu(\\s+\\w+){5}")) %>%
  unnest(word_after_capitu) %>% 
  filter(!is.na(word_after_capitu)) %>%
  select(word_after_capitu) %>%
  print()

```

Context before Capitu (testing with 5)
```{r}
dom_casmurro_pages %>%
  mutate(word_before_capitu = str_extract_all(text, "(\\b\\w+\\s+){5}Capitu")) %>%
  unnest(word_before_capitu) %>% 
  filter(!is.na(word_before_capitu)) %>%
  select(word_before_capitu) %>%
  print()
```


Context before and after Capitu
```{r}
dom_casmurro_pages %>%
  mutate(context_around_capitu = str_extract_all(text, "(\\b\\w+\\s+){5}Capitu(\\s+\\w+){5}")) %>%
  unnest(context_around_capitu) %>% 
  filter(!is.na(context_around_capitu)) %>%
  select(context_around_capitu) %>%
  print()
```

### Use of stop wordlist

To make it easier to access target words (`Capitu` and `Escobar` at an early stage of the analysis), a stop word list will be added in order to eliminate common words. 
First, I tried to include an external stop word list. For some reason, when I tried to get a new ds filtering only the words that were not included in that list, I was getting weird results. It did show that I had fewer observations, which was the expected, but when I went to check the new ds, it still contained words that were on the list. 
After trying different approaches and doing some research on different packages, I discovered there is a Portuguese stop word list included in the `tm` package which seemed to have all the same words I wanted to add with that other stop word list, so I decided to use this one.

```{r}
# install stopwords package because the older one did not take out a specific conjugation of the ver "to be" in portuguese
install.packages("stopwords")
library("stopwords")

?stopwords
?list_edit

# checking stop wordlist (there were three different sources, I chose the nltk because it takes out the verb "é")
head(stopwords::stopwords("pt", source = "nltk"), 20)

# run stopwords
stopwords(language = "pt", source = "nltk", simplify = TRUE)

# create own stop wordlist based on stopwords so that i can add missing words
stopwords_pt <- stopwords("pt", source = "nltk")

stopwords_dom_casmurro <- c(stopwords_pt, "capítulo")

# trying another package (tm)
dom_casmurro_words_filtered <- dom_casmurro_words %>%
  filter(!word %in% stopwords("pt", source = "nltk"))

# word count without common words
dom_casmurro_words_filtered %>% 
  count(word, sort = TRUE)

nrow(dom_casmurro_words)
nrow(dom_casmurro_words_filtered)
```

By using these stopwords, I could see more how `Capitu` is the most none common word used throughout the book.

```{r}
dom_casmurro_words_filtered %>% 
  count(word, sort = TRUE) %>% 
  filter (n > 50) %>% 
  ggplot(aes(label = word, size = n)) +
  geom_text_wordcloud() +
  scale_size_area(max_size = 10) + 
  theme_minimal()
```

## Saving data
```{r}
write_csv(dom_casmurro_pages, "data_pages.csv")

write_csv(dom_casmurro_words, "data_words.csv")
```


## Experiment with POS tagger
### Download packages
```{r}
install.packages("udpipe")
install.packages("flextable")
install.packages("here")
# install klippy for copy-to-clipboard button in code chunks
install.packages("remotes")
remotes::install_github("rlesur/klippy")
```

### Load packages
```{r}
library(udpipe)
library(flextable)
# activate klippy for copy-to-clipboard button
klippy::klippy()
```

### Downloading the pt-model
For some reason, it says on the website that there is a model for pt-br, but when I try to download it, it says it does not exist. Since this is and older text, perhaps it might also works with pt-pt since there are some small differences.
```{r}
#port   <- udpipe_download_model(language = "portuguese-bosque")
```

### Load model
```{r}
port <- udpipe_load_model(file = ("portuguese-bosque-ud-2.5-191206.udpipe"))
```

### Pos-tag the text
```{r}
dom_casmurro_pos <- udpipe::udpipe_annotate(port, x = dom_casmurro_text) %>% 
  as.data.frame()

?str_squish
```

```{r}
sessionInfo()
```


